{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Notebook for deep-icc\n",
    "\n",
    "All cells up to **Model Setup** must be run for model training to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall --yes --quiet osqp\n",
    "# !pip install scikit-survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, Sequence, Tuple, Optional, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from skimage.transform import resize\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v2.summary as summary\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing my own function from external file\n",
    "from patient_data_split import pat_train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions\n",
    "This code is adapted from Sebastian Pölsterl's tutorial on Survival Analysis for Deep Learning Tutorial for TensorFlow 2 https://k-d-w.org/blog/2019/07/survival-analysis-for-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_riskset(time: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute mask that represents each sample's risk set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed event time sorted in descending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    risk_set : np.ndarray, shape=(n_samples, n_samples)\n",
    "        Boolean matrix where the `i`-th row denotes the\n",
    "        risk set of the `i`-th instance, i.e. the indices `j`\n",
    "        for which the observer time `y_j >= y_i`.\n",
    "    \"\"\"\n",
    "    assert time.ndim == 1, \"expected 1D array\"\n",
    "\n",
    "    # sort in descending order\n",
    "    o = np.argsort(-time, kind=\"mergesort\")\n",
    "    \n",
    "    # Initialize risk set \n",
    "    n_samples = len(time)\n",
    "    risk_set = np.zeros((n_samples, n_samples), dtype=np.bool_)\n",
    "    \n",
    "    for i_org, i_sort in enumerate(o):\n",
    "        ti = time[i_sort]\n",
    "        k = i_org\n",
    "        while k < n_samples and ti == time[o[k]]:\n",
    "            k += 1\n",
    "        risk_set[i_sort, o[:k]] = True\n",
    "    return risk_set\n",
    "\n",
    "\n",
    "class InputFunction:\n",
    "    \"\"\"Callable input function that computes the risk set for each batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : np.ndarray, shape=(n_samples, height, width)\n",
    "        Image data.\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed time.\n",
    "    event : np.ndarray, shape=(n_samples,)\n",
    "        Event indicator.\n",
    "    batch_size : int, optional, default=64\n",
    "        Number of samples per batch.\n",
    "    drop_last : int, optional, default=False\n",
    "        Whether to drop the last incomplete batch.\n",
    "    shuffle : bool, optional, default=False\n",
    "        Whether to shuffle data.\n",
    "    seed : int, optional, default=89\n",
    "        Random number seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: np.ndarray,\n",
    "                 time: np.ndarray,\n",
    "                 event: np.ndarray,\n",
    "                 batch_size: int = 64,\n",
    "                 drop_last: bool = False,\n",
    "                 shuffle: bool = False,\n",
    "                 seed: int = 89) -> None:\n",
    "        # If image is 3D, reduce dimension to 2D\n",
    "        if images.ndim == 3:\n",
    "            images = images[..., np.newaxis]\n",
    "        self.images = images\n",
    "        self.time = time\n",
    "        self.event = event\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of samples.\"\"\"\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def steps_per_epoch(self) -> int:\n",
    "        \"\"\"Number of batches for one epoch.\"\"\"\n",
    "        return int(np.floor(self.size() / self.batch_size))\n",
    "\n",
    "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Compute risk set for samples in batch.\n",
    "        \n",
    "        Args:\n",
    "            index - indices for the batch\n",
    "\n",
    "        Returns:\n",
    "            images - numpy array of images in the batch\n",
    "            labels - dictionary of tuples (str, numpy array) with event, time, and riskset labels\n",
    "        \n",
    "        \"\"\"\n",
    "        time = self.time[index]\n",
    "        event = self.event[index]\n",
    "        images = self.images[index]\n",
    "\n",
    "        # Create dictionary of labels for the batch samples\n",
    "        labels = {\n",
    "            \"label_event\": event.astype(np.int32),\n",
    "            \"label_time\": time.astype(np.float32),\n",
    "            \"label_riskset\": _make_riskset(time)\n",
    "        }\n",
    "        return images, labels\n",
    "\n",
    "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
    "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
    "        index = np.arange(self.size())\n",
    "        rnd = np.random.RandomState(self.seed)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(index)\n",
    "        for b in range(self.steps_per_epoch()):\n",
    "            start = b * self.batch_size\n",
    "            idx = index[start:(start + self.batch_size)]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "        if not self.drop_last:\n",
    "            start = self.steps_per_epoch() * self.batch_size\n",
    "            idx = index[start:]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
    "        \"\"\"Return shapes of data returned by `self._iter_data`.\n",
    "        \n",
    "        Returns:\n",
    "            images - tf.TensorShape, shape specification for images\n",
    "            labels - dictionary of (str, tf.TensorShape), shape specification for labels\n",
    "        \"\"\"\n",
    "        batch_size = self.batch_size if self.drop_last else None\n",
    "        h, w, c = self.images.shape[1:]\n",
    "        images = tf.TensorShape([batch_size, h, w, c])\n",
    "\n",
    "        labels = {k: tf.TensorShape((batch_size,))\n",
    "                  for k in (\"label_event\", \"label_time\")}\n",
    "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
    "        return images, labels\n",
    "\n",
    "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
    "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
    "        labels = {\"label_event\": tf.int32,\n",
    "                  \"label_time\": tf.float32,\n",
    "                  \"label_riskset\": tf.bool}\n",
    "        return tf.float32, labels\n",
    "\n",
    "    def _make_dataset(self) -> tf.data.Dataset:\n",
    "        \"\"\"Create dataset from generator.\"\"\"\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            self._iter_data,\n",
    "            self._get_dtypes(),\n",
    "            self._get_shapes()\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    def __call__(self) -> tf.data.Dataset:\n",
    "        return self._make_dataset()\n",
    "\n",
    "\n",
    "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
    "\n",
    "    Note that only risk scores relative to each other matter.\n",
    "    If minimum risk score is negative, we shift scores so minimum\n",
    "    is at zero.\n",
    "    \"\"\"\n",
    "    x_min = tf.reduce_min(x, axis=0)\n",
    "    c = tf.zeros_like(x_min)\n",
    "    norm = tf.where(x_min < 0, -x_min, c)\n",
    "    return x + norm\n",
    "\n",
    "\n",
    "def logsumexp_masked(risk_scores: tf.Tensor,\n",
    "                     mask: tf.Tensor,\n",
    "                     axis: int = 0,\n",
    "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
    "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\n",
    "    \n",
    "    Args:\n",
    "        risk_scores - tf.Tensor of predicted outputs of CoxPH, must be 2D\n",
    "        mask - numpy array of boolean values with risk sets in rows, shape = (n_samples, n_samples)\n",
    "        axis - int indicating which axis to perform sum across, should be axis samples is on (?)\n",
    "        keepdims - bool, wheter to retain reduced dimensions in calculations\n",
    "    \n",
    "    Return:\n",
    "        output - tf.Tensor logsumexp for risk scores\n",
    "    \"\"\"\n",
    "    risk_scores.shape.assert_same_rank(mask.shape)\n",
    "\n",
    "    with tf.name_scope(\"logsumexp_masked\"):\n",
    "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
    "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
    "        # for numerical stability, substract the maximum value\n",
    "        # before taking the exponential\n",
    "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
    "        risk_scores_shift = risk_scores_masked - amax\n",
    "\n",
    "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
    "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
    "        output = amax + tf.math.log(exp_sum)\n",
    "        if not keepdims:\n",
    "            output = tf.squeeze(output, axis=axis)\n",
    "    return output\n",
    "\n",
    "\n",
    "class CoxPHLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)            \n",
    "\n",
    "    def call(self,\n",
    "             y_true: Sequence[tf.Tensor],\n",
    "             y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Compute loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : list|tuple of tf.Tensor\n",
    "            The first element holds a binary vector where 1\n",
    "            indicates an event and 0 indicates censoring.\n",
    "            The second element holds the riskset, a\n",
    "            boolean matrix where the `i`-th row denotes the\n",
    "            risk set of the `i`-th instance, i.e. the indices `j`\n",
    "            for which the observer time `y_j >= y_i`.\n",
    "            Both must be rank 2 tensors.\n",
    "        y_pred : tf.Tensor\n",
    "            The predicted outputs. Must be a rank 2 tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : tf.Tensor\n",
    "            Loss for each instance in the batch.\n",
    "        \"\"\"\n",
    "        event, riskset = y_true\n",
    "        predictions = y_pred\n",
    "\n",
    "        # Input checking\n",
    "        pred_shape = predictions.shape\n",
    "        if pred_shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"be 2.\" % pred_shape.ndims)\n",
    "\n",
    "        if pred_shape[1] is None:\n",
    "            raise ValueError(\"Last dimension of predictions must be known.\")\n",
    "\n",
    "        if pred_shape[1] != 1:\n",
    "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\n",
    "                             \"(received %s) must be 1.\" % pred_shape[1])\n",
    "\n",
    "        if event.shape.ndims != pred_shape.ndims:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"equal rank of event (received %s)\" % (\n",
    "                pred_shape.ndims, event.shape.ndims))\n",
    "\n",
    "        if riskset.shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\n",
    "                             \"be 2.\" % riskset.shape.ndims)\n",
    "\n",
    "        event = tf.cast(event, predictions.dtype)\n",
    "        # Normalize risk scores\n",
    "        predictions = safe_normalize(predictions)\n",
    "\n",
    "        # More input checking\n",
    "        with tf.name_scope(\"assertions\"):\n",
    "            assertions = (\n",
    "                tf.debugging.assert_less_equal(event, 1.),\n",
    "                tf.debugging.assert_greater_equal(event, 0.),\n",
    "                tf.debugging.assert_type(riskset, tf.bool)\n",
    "            )\n",
    "\n",
    "        # move batch dimension to the end so predictions get broadcast\n",
    "        # row-wise when multiplying by riskset\n",
    "        pred_t = tf.transpose(predictions)\n",
    "        # compute log of sum over risk set for each row\n",
    "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\n",
    "        assert rr.shape.as_list() == predictions.shape.as_list()\n",
    "\n",
    "        losses = tf.math.multiply(event, rr - predictions)\n",
    "\n",
    "        return losses\n",
    "    \n",
    "\n",
    "class CindexMetric:\n",
    "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
    "\n",
    "    def reset_states(self) -> None:\n",
    "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
    "        self._data = {\n",
    "            \"label_time\": [],\n",
    "            \"label_event\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "\n",
    "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
    "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : dict\n",
    "            Must have two items:\n",
    "            `label_time`, a tensor containing observed time for one batch,\n",
    "            and `label_event`, a tensor containing event indicator for one batch.\n",
    "        y_pred : tf.Tensor\n",
    "            Tensor containing predicted risk score for one batch.\n",
    "        \"\"\"\n",
    "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
    "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
    "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
    "\n",
    "    def result(self) -> Dict[str, float]:\n",
    "        \"\"\"Computes the concordance index across collected values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        metrics : dict\n",
    "            Computed metrics.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for k, v in self._data.items():\n",
    "            data[k] = np.concatenate(v)\n",
    "\n",
    "        results = concordance_index_censored(\n",
    "            data[\"label_event\"] == 1,\n",
    "            data[\"label_time\"],\n",
    "            data[\"prediction\"])\n",
    "\n",
    "        result_data = {}\n",
    "        names = (\"cindex\", \"concordant\", \"discordant\", \"tied_risk\")\n",
    "        for k, v in zip(names, results):\n",
    "            result_data[k] = v\n",
    "\n",
    "        return result_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndEvaluateModel:\n",
    "    \"\"\"Model with train and evaluate functions for deep survival analysis\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras Model, model to train\n",
    "    model_dir : pathlib.PosixPat, Directory to store training checkpoints\n",
    "    train_dataset : InputFunction, training data set up with InputFunction class\n",
    "    eval_dataset : InputFunction, validation data set up with InputFunction class\n",
    "    learning_rate: float, learning rate to set optimizer with\n",
    "    num_epochs: int, number of training epochs to run\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
    "                 learning_rate, num_epochs):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.train_ds = train_dataset\n",
    "        self.val_ds = eval_dataset\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        # self.optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "        self.loss_fn = CoxPHLoss()\n",
    "\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_cindex_metric = CindexMetric()\n",
    "\n",
    "    @tf.function\n",
    "    def train_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            # pass input through the model\n",
    "            logits = self.model(x, training=True)\n",
    "            # caclulate CPH loss\n",
    "            train_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=logits)\n",
    "\n",
    "        # calculate gradient for weight update \n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        '''Main training function to call with model.'''\n",
    "        \n",
    "        # Set up training checkpoints for saving the model\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(0, dtype=tf.int64),\n",
    "            optimizer=self.optimizer,\n",
    "            model=self.model)\n",
    "        ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, str(self.model_dir), max_to_keep=2)\n",
    "\n",
    "        if ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
    "\n",
    "        train_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"train\"))\n",
    "        val_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"valid\"))\n",
    "\n",
    "        # beginning training\n",
    "        for epoch in range(self.num_epochs):\n",
    "            with train_summary_writer.as_default():\n",
    "                self.train_one_epoch(ckpt.step)\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            with val_summary_writer.as_default():\n",
    "                self.evaluate(ckpt.step)\n",
    "\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
    "\n",
    "    def train_one_epoch(self, step_counter):\n",
    "        for x, y in self.train_ds:\n",
    "            train_loss, logits = self.train_one_step(\n",
    "                x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "\n",
    "            step = int(step_counter)\n",
    "            if step == 0:\n",
    "                # see https://stackoverflow.com/questions/58843269/display-graph-using-tensorflow-v2-0-in-tensorboard\n",
    "                func = self.train_one_step.get_concrete_function(\n",
    "                    x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "                summary_ops_v2.graph(func.graph, step=0)\n",
    "\n",
    "            # Update training metric.\n",
    "            self.train_loss_metric.update_state(train_loss)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                # Display metrics\n",
    "                mean_loss = self.train_loss_metric.result()\n",
    "                print(f\"step {step}: mean loss = {mean_loss:.4f}\")\n",
    "                # save summaries\n",
    "                summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
    "                # Reset training metrics\n",
    "                self.train_loss_metric.reset_states()\n",
    "\n",
    "            step_counter.assign_add(1)\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        val_logits = self.model(x, training=False)\n",
    "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
    "        return val_loss, val_logits\n",
    "\n",
    "    def evaluate(self, step_counter):\n",
    "        self.val_cindex_metric.reset_states()\n",
    "        \n",
    "        for x_val, y_val in self.val_ds:\n",
    "            val_loss, val_logits = self.evaluate_one_step(\n",
    "                x_val, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
    "\n",
    "            # Update val metrics\n",
    "            self.val_loss_metric.update_state(val_loss)\n",
    "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
    "\n",
    "        val_loss = self.val_loss_metric.result()\n",
    "        summary.scalar(\"loss\",\n",
    "                       val_loss,\n",
    "                       step=step_counter)\n",
    "        self.val_loss_metric.reset_states()\n",
    "        \n",
    "        val_cindex = self.val_cindex_metric.result()\n",
    "        for key, value in val_cindex.items():\n",
    "            summary.scalar(key, value, step=step_counter)\n",
    "\n",
    "        print(f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCCA Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for development\n",
    "FILESTOLOAD = 2888 # 2888 is all of them\n",
    "imdim_from_preprocessing = 256 # must match opt.ImageSize in image preprocessing configuration files\n",
    "train_valid_split = 0.8 # where to split data for training and validation\n",
    "random_seed = 16 # used in train-validation splitting\n",
    "dropout_seed = 16 # used in dropout layers in kt_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total label count (should be 2888): (2888,)\n",
      "First RFS time label (should be 0.6): 0.6\n"
     ]
    }
   ],
   "source": [
    "# Path to CSVs that connect patient id to slices and rfs label\n",
    "zero_info_path = \"/Users/katyscott/Documents/ICC/Data/Labels/\" + str(imdim_from_preprocessing) +\"/RFS_all_tumors_zero.csv\"\n",
    "\n",
    "# Reading in info for zero background images\n",
    "info = pd.read_csv(zero_info_path)\n",
    "image_fnames = np.asarray(info.iloc[:, 0])\n",
    "pat_num = np.asarray(info.iloc[:, 1])\n",
    "slice_num = np.asarray(info.iloc[:, 2])\n",
    "rfs_event = np.asarray(info.iloc[:, 3])\n",
    "rfs_time = np.asarray(info.iloc[:, 4])\n",
    "\n",
    "# Confirming loading in happened correctly\n",
    "print(\"Total label count (should be 2888):\", rfs_event.shape)\n",
    "print(\"First RFS time label (should be 0.6):\", rfs_time[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2888/2888 [24:53<00:00,  1.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fc490723070>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAAD8CAYAAACVSwr3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAQvklEQVR4nO3dXWxc9Z3G8e/PHtt5cRzb2JiQEF7aECkVgkYWRAVVXSFa4CZUahG9KKhCSi9SqZW6F5QK0csu2rZSpQWJqqjpii2L+kakwm5TVKlFKi0OoeEdAg6QKJCwpElDXhzbv73wSTrk72An9mSc8v1Ioznzn3M8zxxFD+ecOecQmYkk1WtpdgBJc4/FIKlgMUgqWAySChaDpILFIKnQsGKIiOsj4uWI2BYRdzTqcyTNvmjEeQwR0Qq8AlwH7ACeAr6UmS/M+odJmnWN2mK4EtiWma9n5gjwELC2QZ8laZbVGvR3lwJv1b3eAVx1spn7+vryoosualAUSQCbN29+NzP7pzNvo4phShGxDlgHsHz5coaGhpoVRfpIiIg3pjtvo3YldgIX1L1eVo0dl5n3Z+ZgZg7290+rxCSdIY0qhqeAFRFxcUS0A7cAGxv0WZJmWUN2JTJzNCK+Bvwv0Ao8kJnPN+KzJM2+hh1jyMxHgUcb9fclNY5nPkoqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySChaDpILFIKlgMUgqWAySCrWZLBwR24G/A2PAaGYORkQv8N/ARcB24ObM3DuzmJLOpNnYYviXzLwiMwer13cAj2fmCuDx6rWks0gjdiXWAhuq6Q3ATQ34DEkNNNNiSOC3EbE5ItZVYwOZuauafhsYmGzBiFgXEUMRMbRnz54ZxpA0m2Z0jAG4JjN3RsS5wKaIeKn+zczMiMjJFszM+4H7AQYHByedR1JzzGiLITN3Vs+7gV8BVwLvRMQSgOp590xDSjqzTrsYImJhRCw6Ng18FngO2AjcVs12G/DITENKOrNmsisxAPwqIo79nf/KzP+JiKeAhyPiduAN4OaZx5R0Jp12MWTm68Dlk4z/H3DtTEJJai7PfJRUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVLBYpBUmLIYIuKBiNgdEc/VjfVGxKaIeLV67qnGIyJ+GBHbImJrRKxuZHhJjTGdLYafANefMHYH8HhmrgAer14D3ACsqB7rgPtmJ6akM2nKYsjMPwDvnTC8FthQTW8Abqob/2lOeBLojogls5RV0hlyuscYBjJzVzX9NjBQTS8F3qqbb0c1JuksMuODj5mZQJ7qchGxLiKGImJoz549M40haRadbjG8c2wXoXreXY3vBC6om29ZNVbIzPszczAzB/v7+08zhqRGON1i2AjcVk3fBjxSN35r9evEGmBf3S6HpLNEbaoZIuJnwGeAvojYAdwNfBd4OCJuB94Abq5mfxS4EdgGHAS+0oDMkhpsymLIzC+d5K1rJ5k3gfUzDSWpuTzzUVLBYpBUsBgkFSwGSQWLQVLBYpBUsBgkFSwGSQWLQVM6cuQIXuj20TLlmY/66Nq0aRP79u3j/fff55VXXuHqq6/m6NGj3HDDDbS3tzc7nhrIYtCkNm3axD333MPo6CgDAwMsXLiQ3/zmNxw+fJiXXnqJAwcOcPnll/OFL3yh2VHVABaDCk8//TSPPfYYH//4xzl06BCjo6OMjY3R1tZGrVZj69atHDhwgDfffJNly5axZs2aZkfWLLMY9AGvv/46GzZsYNGiRXR0dDB//nzGx8c5evQoR48e5ciRI7S1tTEwMEBnZye//vWv6e7uZuXKlUREs+NrlnjwUceNjo6ya9cuOjo6OHToEG+++SZvv/02Y2Nj1GoT/w3p6OgA4NChQ9RqNc455xx+/vOfMzw8zMTFtfpn4BaDjtu7dy+33norS5Ysoauri7a2Nvr6+ujq6mJ8fJzMpFarMTo6yuHDh2lra6O/v5+WlhYefPBBvvjFL9Lb28u5557b7K+iGbIYBMC9997La6+9xnvvvUetVmPRokUsXbqUyy67jL6+Pvbt28fBgwdpa2vjwgsvpKWlhba2Nrq7u1mwYAERwSOPPEJvby/XXnstl1xySbO/kmbAYhAAd999Ny0tLaxevZply5bR1dVFb28vPT099Pb2Mj4+TkTQ2dlJX18fHR0d7N27l5GREUZGRmhra+PIkSM8+eST/PGPf2T9+vVcddVVzf5aOk0eYxAAw8PDDA0Ncckll9Df38/ixYtpb2/n6NGjjI6O0tHRQXd3N8uXL2f58uX09fWxePFiarUaf/vb39i+fTtbtmxhy5YtvPDCC9x11128+uqrzf5aOk1uMQiAzs5OMpOBgQFqtRrnn38+8+bNo6uri9bWVrq6uli8eDEDAwN0d3czPj5Oe3s7ixcvZt++fezbt4/zzjuPzs5Oenp6ABgZGWF8fJyWFv/7c7aJuXAkeXBwMIeGhpodQ8C7777Lli1b6Ozs5OjRoxw4cICRkRE6OjpYsmQJAwMDzJ8/n7GxMQ4cOHD8gOThw4cZHh5meHj4+BbEu+++y5133sm5557L+eef3+yv9pEXEZszc3A687rFoA/o6+vjuuuuY+/evezfv5/9+/ezadMmFixYwNjYGAcPHiQiWLhwIYsWLWLevHm0tbXR3t7OeeedR0dHB8PDw2zfvp2DBw9y11130dXVxYMPPtjsr6ZTYDFoUj09PfT09DA+Pg7AE088wa5du9ixYwdtbW2sXLmS7u5u2trajh94HBkZob29ncsuu4yFCxeyefNmdu/ePcUnaS6yGPShWlpaWLVqFRdffDGbN2/mT3/6E/39/SxYsIBarUZLSwsRcXyX4tivGOeccw59fX1ceumlzJ8/v9lfQ6fIYtCUWltb6ezs5JprruFTn/oUEUFEHN+1OHZOw7HTpEdGRjh8+DC1Wo3e3l7mzZvX7K+gU2QxaNpaW1tpbW39wOsTL78eHR3l4MGDrFy5kpUrV57piJol/o6kWVWr1Twl+p+AxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6TClMUQEQ9ExO6IeK5u7DsRsTMinqkeN9a9962I2BYRL0fE5xoVXFLjTGeL4SfA9ZOM/yAzr6gejwJExCrgFuAT1TL3RkTrJMtKmsOmLIbM/APw3jT/3lrgocw8kpnDwDbgyhnkk9QEMznG8LWI2FrtavRUY0uBt+rm2VGNFSJiXUQMRcSQ/8NUaW453WK4D/gYcAWwC/jeqf6BzLw/Mwczc7C/v/80Y0hqhNMqhsx8JzPHMnMc+BH/2F3YCVxQN+uyakzSWeS0iiEiltS9/Dxw7BeLjcAtEdERERcDK4C/zCyipDNtyhu1RMTPgM8AfRGxA7gb+ExEXAEksB34KkBmPh8RDwMvAKPA+swca0hySQ3j7eOlj4hTuX28Zz5KKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKlgMkgoWg6SCxSCpYDFIKkxZDBFxQUT8PiJeiIjnI+Lr1XhvRGyKiFer555qPCLihxGxLSK2RsTqRn8JSbNrOlsMo8A3M3MVsAZYHxGrgDuAxzNzBfB49RrgBmBF9VgH3DfrqSU11JTFkJm7MvPpavrvwIvAUmAtsKGabQNwUzW9FvhpTngS6I6IJbMdXFLjnNIxhoi4CPgk8GdgIDN3VW+9DQxU00uBt+oW21GNSTpLTLsYIqIT+AXwjczcX/9eZiaQp/LBEbEuIoYiYmjPnj2nsqikBptWMUREGxOl8GBm/rIafufYLkL1vLsa3wlcULf4smrsAzLz/swczMzB/v7+080vqQGm86tEAD8GXszM79e9tRG4rZq+DXikbvzW6teJNcC+ul0OSWeB2jTmuRr4MvBsRDxTjd0JfBd4OCJuB94Abq7eexS4EdgGHAS+MpuBJTXelMWQmU8AcZK3r51k/gTWzzCXpCbyzEdJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUsFikFSwGCQVLAZJBYtBUmHKYoiICyLi9xHxQkQ8HxFfr8a/ExE7I+KZ6nFj3TLfiohtEfFyRHyukV9A0uyrTWOeUeCbmfl0RCwCNkfEpuq9H2Tmv9fPHBGrgFuATwDnA7+LiEszc2w2g0tqnCm3GDJzV2Y+XU3/HXgRWPohi6wFHsrMI5k5DGwDrpyNsJLOjFM6xhARFwGfBP5cDX0tIrZGxAMR0VONLQXeqltsB5MUSUSsi4ihiBjas2fPqSeX1DDTLoaI6AR+AXwjM/cD9wEfA64AdgHfO5UPzsz7M3MwMwf7+/tPZVFJDTatYoiINiZK4cHM/CVAZr6TmWOZOQ78iH/sLuwELqhbfFk1JuksMZ1fJQL4MfBiZn6/bnxJ3WyfB56rpjcCt0RER0RcDKwA/jJ7kSU12nR+lbga+DLwbEQ8U43dCXwpIq4AEtgOfBUgM5+PiIeBF5j4RWO9v0hIZ5fIzGZnICL2AO8D7zY7yzT0cXbkhLMnqzln32RZL8zMaR3QmxPFABARQ5k52OwcUzlbcsLZk9Wcs2+mWT0lWlLBYpBUmEvFcH+zA0zT2ZITzp6s5px9M8o6Z44xSJo75tIWg6Q5ounFEBHXV5dnb4uIO5qd50QRsT0inq0uLR+qxnojYlNEvFo990z1dxqQ64GI2B0Rz9WNTZorJvywWsdbI2L1HMg65y7b/5BbDMyp9XpGboWQmU17AK3Aa8AlQDvwV2BVMzNNknE70HfC2D3AHdX0HcC/NSHXp4HVwHNT5QJuBB4DAlgD/HkOZP0O8K+TzLuq+nfQAVxc/ftoPUM5lwCrq+lFwCtVnjm1Xj8k56yt02ZvMVwJbMvM1zNzBHiIicu257q1wIZqegNw05kOkJl/AN47YfhkudYCP80JTwLdJ5zS3lAnyXoyTbtsP09+i4E5tV4/JOfJnPI6bXYxTOsS7SZL4LcRsTki1lVjA5m5q5p+GxhoTrTCyXLN1fV82pftN9oJtxiYs+t1Nm+FUK/ZxXA2uCYzVwM3AOsj4tP1b+bEttqc+2lnruaqM6PL9htpklsMHDeX1uts3wqhXrOLYc5fop2ZO6vn3cCvmNgEe+fYJmP1vLt5CT/gZLnm3HrOOXrZ/mS3GGAOrtdG3wqh2cXwFLAiIi6OiHYm7hW5scmZjouIhdV9LomIhcBnmbi8fCNwWzXbbcAjzUlYOFmujcCt1VH0NcC+uk3jppiLl+2f7BYDzLH1erKcs7pOz8RR1CmOsN7IxFHV14BvNzvPCdkuYeJo7l+B54/lA84BHgdeBX4H9DYh28+Y2Fw8ysQ+4+0ny8XEUfP/qNbxs8DgHMj6n1WWrdU/3CV183+7yvoycMMZzHkNE7sJW4FnqseNc229fkjOWVunnvkoqdDsXQlJc5DFIKlgMUgqWAySChaDpILFIKlgMUgqWAySCv8P/VPYXavEOZEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to zero background image files\n",
    "zero_image_path = '/Users/katyscott/Documents/ICC/Data/Images/Tumors/' + str(imdim_from_preprocessing) + '/Zero/'\n",
    "\n",
    "images = np.empty((1,imdim_from_preprocessing,imdim_from_preprocessing))\n",
    "file_count = 0\n",
    "for image_file in tqdm(image_fnames):\n",
    "    if file_count >= FILESTOLOAD:\n",
    "        break\n",
    "    else:\n",
    "        file_count += 1\n",
    "    #     print(\"Loading: \", image_file)\n",
    "        # Load in file as an numpy array\n",
    "        img = np.fromfile(zero_image_path + image_file)\n",
    "        # Reshape image from 1D to 2D array\n",
    "        img_2D = np.reshape(img, (imdim_from_preprocessing,imdim_from_preprocessing))\n",
    "        # Add third dimension so image can be added to main images array\n",
    "        img_final_3D = np.reshape(img_2D, (1,) + img_2D.shape)\n",
    "        images = np.append(images, img_final_3D, axis=0)\n",
    "\n",
    "images = np.delete(images, 0, axis=0)\n",
    "\n",
    "# Confirming images loaded in properly\n",
    "plt.imshow(images[0], cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:  (2557, 256, 256)\n",
      "Training time labels:  (2557,)\n",
      "Training event labels:  (2557,)\n",
      "Validation set:  (686, 256, 256)\n",
      "Validation time labels:  (686,)\n",
      "Validation event labels:  (686,)\n"
     ]
    }
   ],
   "source": [
    "# Splitting samples by patient and censoring code\n",
    "train_slice_indices, val_slice_indices = pat_train_test_split(pat_num[:FILESTOLOAD], rfs_event[:FILESTOLOAD], train_valid_split, random_seed)\n",
    "\n",
    "# Get training images\n",
    "train_slices = images[train_slice_indices,:,:]#[:][:]\n",
    "train_slices = train_slices.squeeze() # Remove first dim of size 1\n",
    "\n",
    "# Get training labels\n",
    "train_time = rfs_time[train_slice_indices]\n",
    "train_event = rfs_event[train_slice_indices]\n",
    "print(\"Training set: \", train_slices.shape)\n",
    "print(\"Training time labels: \", train_time.shape)\n",
    "print(\"Training event labels: \", train_event.shape)\n",
    "\n",
    "# Get validation images\n",
    "val_slices = images[val_slice_indices,:,:]\n",
    "val_slices = val_slices.squeeze() # Remove first dim of size 1\n",
    "\n",
    "# Get validation labels\n",
    "val_time = rfs_time[val_slice_indices]\n",
    "val_event = rfs_event[val_slice_indices]\n",
    "print(\"Validation set: \", val_slices.shape)\n",
    "print(\"Validation time labels: \", val_time.shape)\n",
    "print(\"Validation event labels: \", val_event.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polsterl tutorial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model layers\n",
    "polsterl_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(6, kernel_size=(5, 5), activation='relu', name='conv_1'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(16, (5, 5), activation='relu', name='conv_2'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(120, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dense(84, activation='relu', name='dense_2'),\n",
    "    tf.keras.layers.Dense(1, activation='linear', name='dense_3')\n",
    "])\n",
    "\n",
    "# Set up training and validation inputs for network\n",
    "polsterl_train_fn = InputFunction(train_slices, train_time, train_event,\n",
    "                  drop_last=False,\n",
    "                  shuffle=True)\n",
    "\n",
    "polsterl_val_fn = InputFunction(val_slices, val_time, val_event)\n",
    "\n",
    "polsterl_trainer = TrainAndEvaluateModel(\n",
    "    model=polsterl_model,\n",
    "    model_dir=Path(\"ckpts-polsterl-100\"),\n",
    "    train_dataset=polsterl_train_fn(),\n",
    "    eval_dataset=polsterl_val_fn(),\n",
    "    learning_rate=0.0003,\n",
    "    num_epochs=100,\n",
    ")\n",
    "\n",
    "polsterl_trainer.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepConvSurv model\n",
    "X. Zhu, J. Yao, and J. Huang. “Deep convolutional neural network for survival analysiswith pathological images”. In:2016 IEEE International Conference on Bioinformatics andBiomedicine (BIBM). 2016, pp. 544–547.DOI:10.1109/BIBM.2016.7822579.7\n",
    "\n",
    "Code adapted from: https://github.com/vanAmsterdam/deep-survival/blob/master/DeepConvSurv.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model layers\n",
    "dcs_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(7, 7), strides = 3, activation='relu', name='conv_1'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides = 2, activation='relu', name='conv_2'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides = 2, activation='relu', name='conv_3'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='relu', name='dense_1'),\n",
    "    tf.keras.layers.Dense(1, activation='linear', name='dense_3')\n",
    "])\n",
    "\n",
    "# Set up training and validation inputs for network\n",
    "dcs_train_fn = InputFunction(train_slices, train_time, train_event,\n",
    "                  batch_size=128,\n",
    "                  drop_last=False,\n",
    "                  shuffle=True)\n",
    "\n",
    "dcs_val_fn = InputFunction(val_slices, val_time, val_event)\n",
    "\n",
    "dcs_trainer = TrainAndEvaluateModel(\n",
    "    model=dcs_model,\n",
    "    model_dir=Path(\"ckpts-dcs-200\"),\n",
    "    train_dataset=dcs_train_fn(),\n",
    "    eval_dataset=dcs_val_fn(),\n",
    "    learning_rate=0.0003,\n",
    "    num_epochs=200,\n",
    ")\n",
    "\n",
    "dcs_trainer.train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KT6 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model layers\n",
    "kt_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(7, 7), strides = 3, activation='selu', name='conv_1'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_1'),\n",
    "    tf.keras.layers.Dropout(0.7, seed=dropout_seed, name='drop_1'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides = 2, activation='selu', name='conv_2'),\n",
    "    tf.keras.layers.Dropout(0.5, seed=dropout_seed, name='drop_2'),\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3, 3), strides = 2, activation='selu', name='conv_3'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2), name='maxpool_2'),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(32, activation='selu', name='dense_1'),\n",
    "    tf.keras.layers.Dense(1, activation='linear', name='dense_2')\n",
    "])\n",
    "\n",
    "# Set up training and validation inputs for network\n",
    "kt_train_fn = InputFunction(train_slices, train_time, train_event,\n",
    "                  batch_size=64,\n",
    "                  drop_last=False,\n",
    "                  shuffle=True)\n",
    "\n",
    "kt_val_fn = InputFunction(val_slices, val_time, val_event)\n",
    "\n",
    "kt_trainer = TrainAndEvaluateModel(\n",
    "    model=kt_model,\n",
    "    model_dir=Path(\"ckpts-kt6-cnn-100\"),\n",
    "    train_dataset=kt_train_fn(),\n",
    "    eval_dataset=kt_val_fn(),\n",
    "    learning_rate=0.0003,\n",
    "    num_epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: mean loss = 26.1056\n",
      "Validation: loss = 3.3270, cindex = 0.5287\n",
      "Validation: loss = 3.1398, cindex = 0.5249\n",
      "Validation: loss = 3.0728, cindex = 0.5119\n",
      "Validation: loss = 3.0167, cindex = 0.5319\n",
      "Validation: loss = 2.9838, cindex = 0.5204\n",
      "step 200: mean loss = 5.3442\n",
      "Validation: loss = 2.9536, cindex = 0.5160\n",
      "Validation: loss = 2.9455, cindex = 0.5183\n",
      "Validation: loss = 2.9313, cindex = 0.5212\n",
      "Validation: loss = 2.9274, cindex = 0.5206\n",
      "Validation: loss = 2.9212, cindex = 0.5187\n",
      "step 400: mean loss = 2.8224\n",
      "Validation: loss = 2.9193, cindex = 0.5186\n",
      "Validation: loss = 2.9163, cindex = 0.5191\n",
      "Validation: loss = 2.9172, cindex = 0.5178\n",
      "Validation: loss = 2.9179, cindex = 0.5203\n",
      "Validation: loss = 2.9171, cindex = 0.5203\n",
      "step 600: mean loss = 2.6971\n",
      "Validation: loss = 2.9171, cindex = 0.5214\n",
      "Validation: loss = 2.9146, cindex = 0.5184\n",
      "Validation: loss = 2.9181, cindex = 0.5196\n",
      "Validation: loss = 2.9191, cindex = 0.5209\n",
      "Validation: loss = 2.9175, cindex = 0.5193\n",
      "step 800: mean loss = 2.6657\n",
      "Validation: loss = 2.9173, cindex = 0.5192\n",
      "Validation: loss = 2.9181, cindex = 0.5194\n",
      "Validation: loss = 2.9186, cindex = 0.5210\n",
      "Validation: loss = 2.9167, cindex = 0.5187\n",
      "Validation: loss = 2.9163, cindex = 0.5210\n",
      "step 1000: mean loss = 2.6484\n",
      "Validation: loss = 2.9183, cindex = 0.5221\n",
      "Validation: loss = 2.9173, cindex = 0.5247\n",
      "Validation: loss = 2.9149, cindex = 0.5236\n",
      "Validation: loss = 2.9173, cindex = 0.5256\n",
      "Validation: loss = 2.9148, cindex = 0.5240\n",
      "step 1200: mean loss = 2.6387\n",
      "Validation: loss = 2.9140, cindex = 0.5224\n",
      "Validation: loss = 2.9156, cindex = 0.5239\n",
      "Validation: loss = 2.9098, cindex = 0.5267\n",
      "Validation: loss = 2.9141, cindex = 0.5251\n",
      "Validation: loss = 2.9205, cindex = 0.5215\n",
      "step 1400: mean loss = 2.6366\n",
      "Validation: loss = 2.9166, cindex = 0.5255\n",
      "Validation: loss = 2.9179, cindex = 0.5258\n",
      "Validation: loss = 2.9198, cindex = 0.5263\n",
      "Validation: loss = 2.9172, cindex = 0.5261\n",
      "Validation: loss = 2.9226, cindex = 0.5269\n",
      "step 1600: mean loss = 2.6286\n",
      "Validation: loss = 2.9176, cindex = 0.5268\n",
      "Validation: loss = 2.9235, cindex = 0.5254\n",
      "Validation: loss = 2.9183, cindex = 0.5271\n",
      "Validation: loss = 2.9149, cindex = 0.5309\n",
      "Validation: loss = 2.9227, cindex = 0.5259\n",
      "step 1800: mean loss = 2.6286\n",
      "Validation: loss = 2.9211, cindex = 0.5276\n",
      "Validation: loss = 2.9184, cindex = 0.5246\n",
      "Validation: loss = 2.9237, cindex = 0.5302\n",
      "Validation: loss = 2.9240, cindex = 0.5287\n",
      "Validation: loss = 2.9253, cindex = 0.5278\n",
      "step 2000: mean loss = 2.6206\n",
      "Validation: loss = 2.9177, cindex = 0.5289\n",
      "Validation: loss = 2.9243, cindex = 0.5302\n",
      "Validation: loss = 2.9201, cindex = 0.5271\n",
      "Validation: loss = 2.9237, cindex = 0.5277\n",
      "Validation: loss = 2.9264, cindex = 0.5259\n",
      "step 2200: mean loss = 2.6142\n",
      "Validation: loss = 2.9148, cindex = 0.5269\n",
      "Validation: loss = 2.9275, cindex = 0.5263\n",
      "Validation: loss = 2.9147, cindex = 0.5360\n",
      "Validation: loss = 2.9193, cindex = 0.5320\n",
      "Validation: loss = 2.9264, cindex = 0.5335\n",
      "step 2400: mean loss = 2.6147\n",
      "Validation: loss = 2.9173, cindex = 0.5321\n",
      "Validation: loss = 2.9232, cindex = 0.5351\n",
      "Validation: loss = 2.9163, cindex = 0.5379\n",
      "Validation: loss = 2.9283, cindex = 0.5354\n",
      "Validation: loss = 2.9333, cindex = 0.5326\n",
      "step 2600: mean loss = 2.6060\n",
      "Validation: loss = 2.9312, cindex = 0.5356\n",
      "Validation: loss = 2.9288, cindex = 0.5307\n",
      "Validation: loss = 2.9348, cindex = 0.5272\n",
      "Validation: loss = 2.9273, cindex = 0.5319\n",
      "Validation: loss = 2.9378, cindex = 0.5344\n",
      "step 2800: mean loss = 2.6032\n",
      "Validation: loss = 2.9303, cindex = 0.5322\n",
      "Validation: loss = 2.9465, cindex = 0.5330\n",
      "Validation: loss = 2.9460, cindex = 0.5307\n",
      "Validation: loss = 2.9172, cindex = 0.5351\n",
      "Validation: loss = 2.9530, cindex = 0.5374\n",
      "step 3000: mean loss = 2.5993\n",
      "Validation: loss = 2.9324, cindex = 0.5352\n",
      "Validation: loss = 2.9288, cindex = 0.5368\n",
      "Validation: loss = 2.9230, cindex = 0.5352\n",
      "Validation: loss = 2.9233, cindex = 0.5376\n",
      "Validation: loss = 2.9381, cindex = 0.5301\n",
      "step 3200: mean loss = 2.5943\n",
      "Validation: loss = 2.9361, cindex = 0.5329\n",
      "Validation: loss = 2.9324, cindex = 0.5320\n",
      "Validation: loss = 2.9489, cindex = 0.5294\n",
      "Validation: loss = 2.9393, cindex = 0.5325\n",
      "Validation: loss = 2.9446, cindex = 0.5286\n",
      "step 3400: mean loss = 2.5875\n",
      "Validation: loss = 2.9344, cindex = 0.5362\n",
      "Validation: loss = 2.9489, cindex = 0.5351\n",
      "Validation: loss = 2.9517, cindex = 0.5301\n",
      "Validation: loss = 2.9330, cindex = 0.5355\n",
      "Validation: loss = 2.9338, cindex = 0.5367\n",
      "step 3600: mean loss = 2.5853\n",
      "Validation: loss = 2.9568, cindex = 0.5318\n",
      "Validation: loss = 2.9273, cindex = 0.5334\n",
      "Validation: loss = 2.9502, cindex = 0.5265\n",
      "Validation: loss = 2.9536, cindex = 0.5369\n",
      "Validation: loss = 2.9370, cindex = 0.5400\n",
      "step 3800: mean loss = 2.5801\n",
      "Validation: loss = 2.9348, cindex = 0.5400\n",
      "Validation: loss = 2.9459, cindex = 0.5361\n",
      "Validation: loss = 2.9473, cindex = 0.5323\n",
      "Validation: loss = 2.9754, cindex = 0.5324\n",
      "Validation: loss = 2.9408, cindex = 0.5365\n",
      "Saved checkpoint for step 4000: ckpts-kt6-100/ckpt-1\n"
     ]
    }
   ],
   "source": [
    "kt_trainer.train_and_evaluate() # kt6-100 run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training output visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Load the Tensorboard notebook extension\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 5663), started 2 days, 21:25:17 ago. (Use '!kill 5663' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9d24ac53face8537\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9d24ac53face8537\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set path to tensorboard installation in virtual environment\n",
    "os.environ['TENSORBOARD_BINARY']='/Users/katyscott/Documents/ICC/venv/bin/tensorboard'\n",
    "\n",
    "%tensorboard --logdir ckpts-kt6-cnn-100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_ktenv",
   "language": "python",
   "name": "new_ktenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
